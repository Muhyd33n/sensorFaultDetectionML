{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d32bb8-2227-4e45-94a3-4fc788bcd0d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import the mlflow autolog. MLflow is used to manage the machine learning lifecycle from initial model development through deployment\n",
    "\n",
    "import mlflow\n",
    "mlflow.pyspark.ml.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f10fdd-fd5e-4ccf-99d0-2a254b908206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718a0a9d-1938-4ec0-a174-a47311d82693",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/FileStore/tables/AVITECH-1.csv', name='AVITECH-1.csv', size=580259, modificationTime=1683497136000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/AVITECH-2.csv', name='AVITECH-2.csv', size=580259, modificationTime=1683497236000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/AVITECH.csv', name='AVITECH.csv', size=580259, modificationTime=1683495554000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/AVITECH.xlsx', name='AVITECH.xlsx', size=813945, modificationTime=1683492442000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/AdventureWorksLT2019.bak', name='AdventureWorksLT2019.bak', size=8511488, modificationTime=1675183879000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/FaultDataset-1.csv', name='FaultDataset-1.csv', size=1703184, modificationTime=1681766199000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/FaultDataset-2.csv', name='FaultDataset-2.csv', size=1703184, modificationTime=1682257960000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/FaultDataset.csv', name='FaultDataset.csv', size=1703184, modificationTime=1682198388000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/FaultDataset.zip', name='FaultDataset.zip', size=288803, modificationTime=1682198344000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/Occupancy_Detection_Data.csv', name='Occupancy_Detection_Data.csv', size=50968, modificationTime=1677684234000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/account-models/', name='account-models/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/account-models1/', name='account-models1/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/accounts/', name='accounts/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/accounts.zip', name='accounts.zip', size=5297592, modificationTime=1677708885000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/activations/', name='activations/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/activations-1.zip', name='activations-1.zip', size=8411369, modificationTime=1677396784000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/activations-2.zip', name='activations-2.zip', size=8411369, modificationTime=1677396917000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/activations.zip', name='activations.zip', size=8411369, modificationTime=1675269099000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/anscombe.json', name='anscombe.json', size=1697, modificationTime=1675184593000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/archive__1_.zip', name='archive__1_.zip', size=8978969, modificationTime=1675184251000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/clinicaltrial_2019/', name='clinicaltrial_2019/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/clinicaltrial_2019.csv', name='clinicaltrial_2019.csv', size=42400056, modificationTime=1680329101000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/clinicaltrial_2019.zip', name='clinicaltrial_2019.zip', size=9707871, modificationTime=1680283552000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/clinicaltrial_2020.zip', name='clinicaltrial_2020.zip', size=10599182, modificationTime=1680283556000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/clinicaltrial_2021/', name='clinicaltrial_2021/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/clinicaltrial_2021.csv', name='clinicaltrial_2021.csv', size=50359696, modificationTime=1682501407000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/clinicaltrial_2021.zip', name='clinicaltrial_2021.zip', size=11508457, modificationTime=1680283598000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/flood-1.csv', name='flood-1.csv', size=128984, modificationTime=1676545063000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/flood-2.csv', name='flood-2.csv', size=128984, modificationTime=1676636213000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/flood.csv', name='flood.csv', size=128984, modificationTime=1676483643000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/iotstream/', name='iotstream/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/iotstream.zip', name='iotstream.zip', size=43891, modificationTime=1677087413000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/logs/', name='logs/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/logs.zip', name='logs.zip', size=18168065, modificationTime=1677708970000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/movies.csv', name='movies.csv', size=494431, modificationTime=1678283586000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/myratings_1_.csv', name='myratings_1_.csv', size=10683, modificationTime=1678283584000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/myrdd.json/', name='myrdd.json/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/pharma/', name='pharma/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/pharma.csv', name='pharma.csv', size=678999, modificationTime=1682198387000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/pharma.zip', name='pharma.zip', size=109982, modificationTime=1680283557000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/ratings.csv', name='ratings.csv', size=2483723, modificationTime=1678283594000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/spark_excel_2_11_0_12_2.jar', name='spark_excel_2_11_0_12_2.jar', size=6222374, modificationTime=1683494461000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/test.json', name='test.json', size=17958, modificationTime=1674668040000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/webpage/', name='webpage/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/webpage-1.zip', name='webpage-1.zip', size=1582, modificationTime=1675874630000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/webpage.zip', name='webpage.zip', size=1582, modificationTime=1675874543000),\n",
       " FileInfo(path='dbfs:/FileStore/tables/webpage_files_all/', name='webpage_files_all/', size=0, modificationTime=0),\n",
       " FileInfo(path='dbfs:/FileStore/tables/webpage_files_jpg/', name='webpage_files_jpg/', size=0, modificationTime=0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The fault dataset was uploaded using the Data UI \n",
    "\n",
    "#Run the databricks utility command to list the datasets in the DBFS. Copy the FaultDataset from the list and read it into Spark Dataframe \n",
    "dbutils.fs.ls(\"/FileStore/tables/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c168b0-026f-45cb-a3dc-13ab7d736f29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f315368-4811-4b69-a5e0-cdbf5b285a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reading the dataset into  Spark DataFrame\n",
    "faultDatasetDF =spark.read.csv(\"/FileStore/tables/FaultDataset.csv\", header = \"true\", inferSchema =\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4349f30c-8e77-405e-bf7b-11cdc349418a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df619bde-c301-4bda-ad05-a44e6412eafe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- 1: double (nullable = true)\n |-- 2: double (nullable = true)\n |-- 3: double (nullable = true)\n |-- 4: double (nullable = true)\n |-- 5: double (nullable = true)\n |-- 6: double (nullable = true)\n |-- 7: double (nullable = true)\n |-- 8: double (nullable = true)\n |-- 9: double (nullable = true)\n |-- 10: double (nullable = true)\n |-- 11: double (nullable = true)\n |-- 12: double (nullable = true)\n |-- 13: double (nullable = true)\n |-- 14: double (nullable = true)\n |-- 15: double (nullable = true)\n |-- 16: double (nullable = true)\n |-- 17: double (nullable = true)\n |-- 18: double (nullable = true)\n |-- 19: double (nullable = true)\n |-- 20: double (nullable = true)\n |-- fault_detected: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#I will also use the print schema to display the names and the datatypes of the columns of the dataset\n",
    "faultDatasetDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a381bf01-6ac6-4504-8311-98760e20f51c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+\n|        1|        2|        3|        4|        5|        6|        7|        8|        9|       10|       11|       12|       13|       14|       15|       16|       17|       18|       19|       20|fault_detected|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+\n|0.3503125|0.3496875|     0.35|0.3459375|   0.3475|0.3459375| 0.341875|0.3434375|    0.355|0.3553125|0.3459375|   0.3525|   0.3575|0.3590625|  0.35875|0.3484375|0.3590625|     0.35|0.3559375|0.3490625|             0|\n|0.5090625| 0.484375| 0.046875| 0.071875|     0.06|0.0634375|   0.0575|0.0546875|0.0559375| 0.058125|0.0628125| 0.065625|0.0640625|0.0634375|0.0534375| 0.084375|0.0615625|  0.05375| 0.076875| 0.056875|             0|\n|0.0928125|   0.0975|0.1096875|   0.1025|  0.09625|0.1053125|  0.09875| 0.098125| 0.091875|0.0909375|  0.09875| 0.103125|      0.1|0.1034375|0.1015625|0.0978125|0.0990625|  0.10375| 0.098125|0.1040625|             0|\n|  0.09375| 0.089375| 0.091875|0.0996875|0.0909375| 0.096875|0.0940625| 0.096875| 0.096875| 0.099375| 0.099375|0.0959375|0.0959375|0.0940625|  0.09125|0.0996875|  0.09375|0.0934375|0.0971875| 0.094375|             0|\n| 0.036875|0.0440625| 0.038125|0.0428125|0.0353125|0.0340625| 0.033125|0.0403125|0.0346875| 0.036875| 0.035625|  0.03625|0.0409375| 0.039375|    0.035| 0.040625|0.0384375| 0.036875|     0.04|0.0371875|             0|\n| 0.135625|0.3034375|  0.13875| 0.140625| 0.126875| 0.130625| 0.139375| 0.143125|0.1290625| 0.140625|0.1340625|0.1396875|0.1384375|0.1453125|0.1453125|0.1496875|0.1440625|0.1359375|0.1453125|  0.14625|             0|\n|0.3446875|  0.35125|0.3353125|0.3471875|  0.34625| 0.348125|0.3478125|0.3521875|   0.3525|  0.35125|0.3571875| 0.360625|0.3640625|  0.36625|0.3640625|0.3634375|   0.3475|  0.35375|   0.1575| 0.351875|             0|\n| 0.036875| 0.035625|  0.03125|   0.0375|0.0390625| 0.034375|0.0315625| 0.031875|0.0378125|0.0321875|0.0371875| 0.038125|    0.035|0.0353125|   0.0325|     0.03|   0.0325|0.0321875|0.0321875|  0.03125|             0|\n|0.0371875| 0.039375| 0.033125|     0.04|  0.04125|  0.03875| 0.035625|0.0384375|0.0378125|0.0365625| 0.033125|0.0365625|  0.03375| 0.034375|0.0346875|  0.04125|0.0365625|    0.035| 0.034375|0.0396875|             0|\n|0.3590625|0.3609375| 0.360625|0.3590625|    0.355|    0.365| 0.355625| 0.358125|   0.3575|0.3578125| 0.355625|0.3584375|0.3521875|0.3459375|0.3521875|0.3509375|   0.3525|  0.35625| 0.353125|0.3540625|             0|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#df.show() is used here to view the dataset. this gives a bigger picture of what the  faultdataset looks like. \n",
    "#Each row contains twenty vibration sensor readings, and the final column identifies whether there was a fault with the machine at the time of the readings.\n",
    "faultDatasetDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d514efc2-90f4-4b49-aa80-1cb8dd9437af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>summary</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th><th>16</th><th>17</th><th>18</th><th>19</th><th>20</th><th>fault_detected</th></tr></thead><tbody><tr><td>count</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td><td>9292</td></tr><tr><td>mean</td><td>0.34162330499354226</td><td>0.34263116121394677</td><td>0.3421213812957383</td><td>0.34213907124407966</td><td>0.3428434405940584</td><td>0.3428279366659492</td><td>0.3427147008179088</td><td>0.34306577566724017</td><td>0.34317299155187225</td><td>0.3439251842983209</td><td>0.3441075320167876</td><td>0.34398424047567727</td><td>0.3441183275936292</td><td>0.3448787599547991</td><td>0.34489994753551373</td><td>0.3458342525828662</td><td>0.34563488753766686</td><td>0.3457525290572536</td><td>0.3465221427034015</td><td>0.346670724817048</td><td>0.5</td></tr><tr><td>stddev</td><td>0.28919489486260785</td><td>0.2890875372793958</td><td>0.28916422490616933</td><td>0.28916356333107296</td><td>0.2889646554403878</td><td>0.2890889899729543</td><td>0.2891948159883224</td><td>0.28919185608065456</td><td>0.2893401858067147</td><td>0.289011538534877</td><td>0.28920014487495876</td><td>0.2890708129465896</td><td>0.28911804701463106</td><td>0.2889821392646809</td><td>0.2891314011350137</td><td>0.2888285654988746</td><td>0.28892040336707314</td><td>0.28915028148431343</td><td>0.2887705775702368</td><td>0.28900135543931055</td><td>0.5000269070362092</td></tr><tr><td>min</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.025</td><td>0.025</td><td>0.025</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.024375</td><td>0.025</td><td>0</td></tr><tr><td>max</td><td>1.0809375</td><td>1.2134375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.2134375</td><td>1.0809375</td><td>1.2134375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1.0809375</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292",
         "9292"
        ],
        [
         "mean",
         "0.34162330499354226",
         "0.34263116121394677",
         "0.3421213812957383",
         "0.34213907124407966",
         "0.3428434405940584",
         "0.3428279366659492",
         "0.3427147008179088",
         "0.34306577566724017",
         "0.34317299155187225",
         "0.3439251842983209",
         "0.3441075320167876",
         "0.34398424047567727",
         "0.3441183275936292",
         "0.3448787599547991",
         "0.34489994753551373",
         "0.3458342525828662",
         "0.34563488753766686",
         "0.3457525290572536",
         "0.3465221427034015",
         "0.346670724817048",
         "0.5"
        ],
        [
         "stddev",
         "0.28919489486260785",
         "0.2890875372793958",
         "0.28916422490616933",
         "0.28916356333107296",
         "0.2889646554403878",
         "0.2890889899729543",
         "0.2891948159883224",
         "0.28919185608065456",
         "0.2893401858067147",
         "0.289011538534877",
         "0.28920014487495876",
         "0.2890708129465896",
         "0.28911804701463106",
         "0.2889821392646809",
         "0.2891314011350137",
         "0.2888285654988746",
         "0.28892040336707314",
         "0.28915028148431343",
         "0.2887705775702368",
         "0.28900135543931055",
         "0.5000269070362092"
        ],
        [
         "min",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.025",
         "0.025",
         "0.025",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.024375",
         "0.025",
         "0"
        ],
        [
         "max",
         "1.0809375",
         "1.2134375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.2134375",
         "1.0809375",
         "1.2134375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1.0809375",
         "1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "4",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "5",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "6",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "7",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "8",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "9",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "10",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "11",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "12",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "13",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "14",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "15",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "16",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "17",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "18",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "19",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "20",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "fault_detected",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This code perform a basic statistics for the columnns of the faultDF. From the result, it was discovered that no null values for any of the columns. The count is the same for all columns. COUNT = 9292\n",
    "faultDatasetDF.describe().display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c83e52f-fe39-4029-88e1-13cd3d909f72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t with null values: 0\n2 \t with null values: 0\n3 \t with null values: 0\n4 \t with null values: 0\n5 \t with null values: 0\n6 \t with null values: 0\n7 \t with null values: 0\n8 \t with null values: 0\n9 \t with null values: 0\n10 \t with null values: 0\n11 \t with null values: 0\n12 \t with null values: 0\n13 \t with null values: 0\n14 \t with null values: 0\n15 \t with null values: 0\n16 \t with null values: 0\n17 \t with null values: 0\n18 \t with null values: 0\n19 \t with null values: 0\n20 \t with null values: 0\nfault_detected \t with null values: 0\n"
     ]
    }
   ],
   "source": [
    "#the count for the dataset is 9292. the for loop below iterate through the columns and tell the number of null in each of the columns. The result is \"0\" for all columns\n",
    "#What this implies is that there is no column with null value.\n",
    "for col in faultDatasetDF.columns:\n",
    "    print(col, \"\\t\", \"with null values:\", faultDatasetDF.filter(faultDatasetDF[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2567318e-86f6-4d53-8791-a49dacc159bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Distribution of Faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b374725-49fb-401a-93ae-9241569059f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import matplotlib and seaborn libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "#Checking the distribution of the dependent variable(fault_detected)\n",
    "\n",
    "fault_distribution = faultDatasetDF.groupBy('fault_detected').count().toPandas()\n",
    "\n",
    "#Create bar plot with count values\n",
    "ax = sns.barplot(x='fault_detected', y='count', data=fault_distribution)\n",
    "\n",
    "plt.title('Distribution of Faults')\n",
    "\n",
    "# Add count values to the bars\n",
    "for i in range(len(fault_distribution)):\n",
    "    count = fault_distribution['count'][i]\n",
    "    ax.text(i, count+1, str(count), ha='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ecb2d7-a97a-4c5d-89d1-5d46b468dcfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Correlation of the Sensor Readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b625043-5f8d-429d-8175-1a5ae1bd9a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import matplotlib and seaborn libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Select the sensor readings columns, exclude the dependent variable\n",
    "sensor_cols = [str(i) for i in range(1,21)]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = faultDatasetDF.select(sensor_cols).toPandas().corr()\n",
    "\n",
    "# Plot the heatmap using seaborn\n",
    "fig, ax = plt.subplots(figsize=(13, 9))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381c1c32-08ce-46b7-bd50-f07f376a0ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3fe9ebb-6adf-420a-ae49-4e0132e2b9db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:18:09 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '9a816063ae9d4755a289437fb44db2a3', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    }
   ],
   "source": [
    "#process the data into correct format using Rformula\n",
    "from pyspark.ml.feature import RFormula\n",
    "preprocess = RFormula(formula = \"fault_detected  ~  . \")\n",
    "faultDatasetDF = preprocess.fit(faultDatasetDF).transform(faultDatasetDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b05fb07-2226-4c7f-9eda-cf483827f4e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Splitting the dataset into Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee6d85c-8eba-460e-84b9-87328092674f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To avoid overfitting issue, the dataset will be splitted into training and test dateset\n",
    "\n",
    "(trainingDF, testDF) =faultDatasetDF.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c649e17c-d77e-4cdb-9cfb-b524e4daa55f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Training The DecisionTreeClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18f68d0-752b-4cf2-b942-449db8d37eba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:18:10 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'd8a8e86c0a0245938a75b91e916b7270', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n2023/08/22 10:18:10 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n2023/08/22 10:18:14 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField('features', VectorUDT(), True)]. Model signature is not logged.\n2023/08/22 10:18:18 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    }
   ],
   "source": [
    "#importing the decisionTreeClassifier from ml library\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "#the label and the features are the ones added to the DF during preprocessing. the feature is the vector of all columns except fault_detected \n",
    "dt_classifier = DecisionTreeClassifier(labelCol = \"label\", featuresCol = \"features\")\n",
    "#fitting the model with the trained dataset\n",
    "model = dt_classifier.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d9db2c-38d1-489a-8deb-e2d0e36353a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbfb311-a3a4-468c-becf-d8e3976df53e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------+--------------------+----------+\n|        1|        2|        3|        4|        5|        6|        7|        8|        9|       10|       11|       12|       13|       14|       15|       16|       17|       18|       19|       20|fault_detected|            features|label| rawPrediction|         probability|prediction|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------+--------------------+----------+\n|0.0253125| 0.039375|0.0528125| 0.056875|   0.0725| 0.041875|0.5109375|0.5628125|    0.605|   0.6025|0.6259375|      0.6|0.6090625|0.6034375| 0.594375|0.6021875| 0.580625|0.5703125|0.0503125| 0.069375|             1|[0.0253125,0.0393...|  1.0|  [12.0,139.0]|[0.07947019867549...|       1.0|\n|  0.02625|   0.0325|0.0296875|0.0353125|0.0328125|0.0265625|  0.03125|0.0328125| 0.029375|0.0284375|0.0334375|0.0321875|0.0296875|   0.0325|0.0284375|0.0259375|  0.03125|  0.02875|0.0328125|0.0334375|             1|[0.02625,0.0325,0...|  1.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n|  0.02625|0.0334375|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|0.0328125| 0.029375|0.0253125| 0.039375|             1|[0.02625,0.033437...|  1.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n|0.0265625|0.0284375|  0.02625|   0.0325|0.0296875|0.0353125|0.0328125|0.0265625|  0.03125|0.0328125| 0.029375|0.0284375|0.0334375|0.0321875|0.0296875|   0.0325|0.0284375|0.0259375|  0.03125|  0.02875|             1|[0.0265625,0.0284...|  1.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n|0.0265625|  0.02875|0.0321875|0.0290625|0.0271875|0.0265625|0.0296875|0.0278125|0.0284375|0.0271875|0.0284375|0.0309375|0.0309375|  0.02875|0.0340625|0.0296875|  0.03125|0.0359375| 0.671875|0.0640625|             1|[0.0265625,0.0287...|  1.0|   [0.0,104.0]|           [0.0,1.0]|       1.0|\n|0.0265625|0.0315625|0.0296875|0.0321875|   0.0275|0.0315625|  0.03625|  0.03375|  0.03375|0.0271875| 0.030625|0.0334375| 0.029375|  0.02625|0.0353125|0.0346875| 0.036875|0.0365625|     0.03|0.0309375|             1|[0.0265625,0.0315...|  1.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n| 0.026875| 0.024375|     0.03|  0.03375|  0.02625|0.0334375|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|             1|[0.026875,0.02437...|  1.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|0.0328125| 0.029375|0.0253125| 0.039375|0.0528125| 0.056875|             1|[0.0271875,0.0275...|  1.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n|0.0271875| 0.029375|0.0284375| 0.031875|  0.02875| 0.026875|0.0315625|0.0321875|0.0353125|0.0303125|0.0328125|     0.03|0.0278125|     0.03|0.0284375| 0.029375|  0.02875|  0.03125|   0.0325|0.0290625|             0|[0.0271875,0.0293...|  0.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n|   0.0275|0.0315625|  0.03625|  0.03375|  0.03375|0.0271875| 0.030625|0.0334375| 0.029375|  0.02625|0.0353125|0.0346875| 0.036875|0.0365625|     0.03|0.0309375|0.0340625| 0.040625|0.0334375|0.0753125|             1|[0.0275,0.0315625...|  1.0|[3155.0,142.0]|[0.95693054291780...|       0.0|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------+--------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#predicting the unseen data with the model we trained above\n",
    "predictions = model.transform(testDF)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a3e83c-9913-4bcc-9f1a-5e972f9753b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|label|prediction|\n+-----+----------+\n|  1.0|       1.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n+-----+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Lets view the first 10 rows of our prediction and lets compare it to our label. This shows less than 50 percent accuracy\n",
    "predictions.select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8082fc9c-a561-40c3-9590-2e9264f9a22b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluating the Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "336d6078-4b71-4335-8933-80fd5f5c4268",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:19:12 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.952432\n"
     ]
    }
   ],
   "source": [
    "#Let's evalaute the accuracy of the whole predictions\n",
    "#The accuracy is 0.952432, which means that 95% of the predictions made by our model on the test dataset are correct, this is a good result.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol =\"label\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print (\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0917fe03-61e7-4c78-8b4b-762e7c3fa026",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ParamGridBuilder & TrainValidationSplit for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c2d315-3c2c-4e61-8ba0-086af3299769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to identify the optimal set of hyperparameters that predict best (maxDepth,impurity, and maxBins), we will define a grid of values for thes parameters\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder \n",
    "#Create grid builder\n",
    "parameters=ParamGridBuilder()\\\n",
    "           .addGrid(dt_classifier.impurity, [\"gini\", \"entropy\"]) \\\n",
    "           .addGrid(dt_classifier.maxDepth, [3, 5,  7])   \\\n",
    "           .addGrid(dt_classifier.maxBins,[16,32,64]).build() \\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4692777c-1d5c-4565-816d-e2db0104041c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "#instatiate TrainValidationSplit to split data. fit the decision tree classifer, the grid defined and the evaluator we earlier defined\n",
    " \n",
    "tvs=TrainValidationSplit()   \\\n",
    "    .setSeed(100).setTrainRatio(0.75)   \\\n",
    "    .setEstimatorParamMaps(parameters)  \\\n",
    "    .setEstimator(dt_classifier)        \\\n",
    "    .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8419982c-5a13-4a14-84fc-f8f6b94a601c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:19:14 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ac1d199a3fe3440e9434ecd7d8a12e25', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n2023/08/22 10:19:14 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n2023/08/22 10:20:12 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2023/08/22 10:21:13 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    }
   ],
   "source": [
    "#Train model using the grid search\n",
    "\n",
    "gridsearchmodel = tvs.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9291076f-7921-423c-87c6-e43f8d79aaad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the best model:\nMaxDepth Parameter: 7\nImpurity Parameter: entropy\nMaxBins Parameter: 64\n"
     ]
    }
   ],
   "source": [
    "#find the best performing model\n",
    "bestModel = gridsearchmodel.bestModel\n",
    " \n",
    " \n",
    "print(\"Parameters for the best model:\")\n",
    "print(\"MaxDepth Parameter: %g\" %bestModel.getMaxDepth())\n",
    "print(\"Impurity Parameter: %s\" %bestModel.getImpurity())\n",
    "print(\"MaxBins Parameter: %g\" %bestModel.getMaxBins())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5a0713-82da-4db2-ae9f-97c8850d0e5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.961081081081081"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make prediction with the test dataset using the bestModel\n",
    "evaluator.evaluate(bestModel.transform(testDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52713d31-e3a2-46f4-92c7-ae3b40c5782f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c0fc33-e31b-4b82-9641-24e74831f518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:22:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a2f1a1e229144e9b9741132d98da191a', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n2023/08/22 10:22:04 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n2023/08/22 10:22:15 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    }
   ],
   "source": [
    "# importing the random forest classifier from ML library\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "#the label and the features are the ones added to the DF during preprocessing. The 'features' is the vector of all columns except fault_detected \n",
    "rf_classifier = RandomForestClassifier(labelCol = \"label\", featuresCol = \"features\")\n",
    "#fitting the model\n",
    "rf_model = rf_classifier.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8589d60f-0a16-4a96-b53a-2c1e2f391801",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluating the RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51331fa-110a-40ca-8397-d4d728e6a52d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------------+--------------------+----------+\n|        1|        2|        3|        4|        5|        6|        7|        8|        9|       10|       11|       12|       13|       14|       15|       16|       17|       18|       19|       20|fault_detected|            features|label|       rawPrediction|         probability|prediction|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------------+--------------------+----------+\n|0.0253125| 0.039375|0.0528125| 0.056875|   0.0725| 0.041875|0.5109375|0.5628125|    0.605|   0.6025|0.6259375|      0.6|0.6090625|0.6034375| 0.594375|0.6021875| 0.580625|0.5703125|0.0503125| 0.069375|             1|[0.0253125,0.0393...|  1.0|[2.35897873729936...|[0.11794893686496...|       1.0|\n|  0.02625|   0.0325|0.0296875|0.0353125|0.0328125|0.0265625|  0.03125|0.0328125| 0.029375|0.0284375|0.0334375|0.0321875|0.0296875|   0.0325|0.0284375|0.0259375|  0.03125|  0.02875|0.0328125|0.0334375|             1|[0.02625,0.0325,0...|  1.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n|  0.02625|0.0334375|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|0.0328125| 0.029375|0.0253125| 0.039375|             1|[0.02625,0.033437...|  1.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n|0.0265625|0.0284375|  0.02625|   0.0325|0.0296875|0.0353125|0.0328125|0.0265625|  0.03125|0.0328125| 0.029375|0.0284375|0.0334375|0.0321875|0.0296875|   0.0325|0.0284375|0.0259375|  0.03125|  0.02875|             1|[0.0265625,0.0284...|  1.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n|0.0265625|  0.02875|0.0321875|0.0290625|0.0271875|0.0265625|0.0296875|0.0278125|0.0284375|0.0271875|0.0284375|0.0309375|0.0309375|  0.02875|0.0340625|0.0296875|  0.03125|0.0359375| 0.671875|0.0640625|             1|[0.0265625,0.0287...|  1.0|[10.1993396945958...|[0.50996698472979...|       0.0|\n|0.0265625|0.0315625|0.0296875|0.0321875|   0.0275|0.0315625|  0.03625|  0.03375|  0.03375|0.0271875| 0.030625|0.0334375| 0.029375|  0.02625|0.0353125|0.0346875| 0.036875|0.0365625|     0.03|0.0309375|             1|[0.0265625,0.0315...|  1.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n| 0.026875| 0.024375|     0.03|  0.03375|  0.02625|0.0334375|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|             1|[0.026875,0.02437...|  1.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|0.0328125| 0.029375|0.0253125| 0.039375|0.0528125| 0.056875|             1|[0.0271875,0.0275...|  1.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n|0.0271875| 0.029375|0.0284375| 0.031875|  0.02875| 0.026875|0.0315625|0.0321875|0.0353125|0.0303125|0.0328125|     0.03|0.0278125|     0.03|0.0284375| 0.029375|  0.02875|  0.03125|   0.0325|0.0290625|             0|[0.0271875,0.0293...|  0.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n|   0.0275|0.0315625|  0.03625|  0.03375|  0.03375|0.0271875| 0.030625|0.0334375| 0.029375|  0.02625|0.0353125|0.0346875| 0.036875|0.0365625|     0.03|0.0309375|0.0340625| 0.040625|0.0334375|0.0753125|             1|[0.0275,0.0315625...|  1.0|[19.0401577490003...|[0.95200788745001...|       0.0|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#use the model to predict the unseen dataset/ test dataset\n",
    "rf_predictions=rf_model.transform(testDF)\n",
    "rf_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b5f41b1-fd88-4c06-bc05-239fbbc15843",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|label|prediction|\n+-----+----------+\n|  1.0|       1.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       1.0|\n|  0.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n+-----+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Lets view the performance of our prediction against our label data\n",
    "rf_predictions.select(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49709656-63a3-4170-83d2-2be3c701e535",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:23:07 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.967928\n"
     ]
    }
   ],
   "source": [
    "#Let's evalaute the accuracy of the whole predictions\n",
    "#The accuracy is  0.967928, which means that about 96.8% of the predictions made by our model on the test dataset are correct, this is a good result.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol =\"label\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "\n",
    "rf_accuracy = rf_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print (\"Accuracy = %g\" % (rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2f830f2-54fa-4fb6-8c0f-ae5ecb5c01af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a parameter grid for tuning the model\n",
    "#to identify the optimal set of hyperparameters(maxDepth,impurity, and maxBins), we will define a grid of values for thes parameters\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder \n",
    "#Create grid builder\n",
    "rf_parameters=ParamGridBuilder().addGrid(rf_classifier.impurity, [\"gini\", \"entropy\"])\\\n",
    "              .addGrid(rf_classifier.maxDepth, [3, 5,  7])\\\n",
    "              .addGrid(rf_classifier.maxBins,[16,32,64]).build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e67838e-84ba-4a27-a871-229deed8cf63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "#Define TrainValidationSplit\n",
    "\n",
    "rf_tvs=TrainValidationSplit()   \\\n",
    "    .setSeed(100).setTrainRatio(0.75)   \\\n",
    "    .setEstimatorParamMaps(rf_parameters)  \\\n",
    "    .setEstimator(rf_classifier)        \\\n",
    "    .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd0e429-d69c-41a7-b73d-26c8c24ce4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:23:09 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '2d94e389cb7d454dad89423d45e196af', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n2023/08/22 10:23:10 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n2023/08/22 10:24:16 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2023/08/22 10:25:20 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    }
   ],
   "source": [
    "#Train model using the grid search\n",
    "\n",
    "rf_gridsearchmodel = rf_tvs.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b700ce-f846-4ae6-a47d-d7936c4d19c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the best model:\nMaxDepth Parameter: 7\nImpurity Parameter: entropy\nMaxBins Parameter: 64\n"
     ]
    }
   ],
   "source": [
    "#find the best performing model\n",
    "rf_bestModel = rf_gridsearchmodel.bestModel\n",
    " \n",
    " \n",
    "print(\"Parameters for the best model:\")\n",
    "print(\"MaxDepth Parameter: %g\" %bestModel.getMaxDepth())\n",
    "print(\"Impurity Parameter: %s\" %bestModel.getImpurity())\n",
    "print(\"MaxBins Parameter: %g\" %bestModel.getMaxBins())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c8e953-fe51-4d7f-a347-cac963f0f753",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the RandomForestClassifier on the test dataset is: 0.9704504504504504\n"
     ]
    }
   ],
   "source": [
    "#make prediction with the test dataset using the bestModel\n",
    "RFCaccuracy= evaluator.evaluate(rf_bestModel.transform(testDF))\n",
    "\n",
    "#print the accuracy \n",
    "print(\"The accuracy of the RandomForestClassifier on the test dataset is:\", RFCaccuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e33066-cfc3-4cac-8888-6e9d9fd5b507",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Train Gradient-Boosted Trees (GBTs) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72678aaf-f026-48d4-92ee-901a6505e56a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:26:14 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f464b69fecdf46e9a245729d648b296f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n2023/08/22 10:26:14 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n2023/08/22 10:26:39 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n"
     ]
    }
   ],
   "source": [
    "#Import Gradient-Boosted Trees (GBTs)\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "GBT_classifier = GBTClassifier(labelCol = \"label\", featuresCol = \"features\")\n",
    "#train the model\n",
    "GBT_model = GBT_classifier.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e462ec77-3a52-4fb7-b407-b4af6e512be6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb710e5-b1fe-4a21-9e59-e1090dcf88f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------------+--------------------+----------+\n|        1|        2|        3|        4|        5|        6|        7|        8|        9|       10|       11|       12|       13|       14|       15|       16|       17|       18|       19|       20|fault_detected|            features|label|       rawPrediction|         probability|prediction|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------------+--------------------+----------+\n|0.0253125| 0.039375|0.0528125| 0.056875|   0.0725| 0.041875|0.5109375|0.5628125|    0.605|   0.6025|0.6259375|      0.6|0.6090625|0.6034375| 0.594375|0.6021875| 0.580625|0.5703125|0.0503125| 0.069375|             1|[0.0253125,0.0393...|  1.0|[-1.5991548750592...|[0.03922937945839...|       1.0|\n|  0.02625|   0.0325|0.0296875|0.0353125|0.0328125|0.0265625|  0.03125|0.0328125| 0.029375|0.0284375|0.0334375|0.0321875|0.0296875|   0.0325|0.0284375|0.0259375|  0.03125|  0.02875|0.0328125|0.0334375|             1|[0.02625,0.0325,0...|  1.0|[1.26581025102566...|[0.92632901866998...|       0.0|\n|  0.02625|0.0334375|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|0.0328125| 0.029375|0.0253125| 0.039375|             1|[0.02625,0.033437...|  1.0|[0.94249676908782...|[0.86818364239217...|       0.0|\n|0.0265625|0.0284375|  0.02625|   0.0325|0.0296875|0.0353125|0.0328125|0.0265625|  0.03125|0.0328125| 0.029375|0.0284375|0.0334375|0.0321875|0.0296875|   0.0325|0.0284375|0.0259375|  0.03125|  0.02875|             1|[0.0265625,0.0284...|  1.0|[0.21355612805055...|[0.60518389144282...|       0.0|\n|0.0265625|  0.02875|0.0321875|0.0290625|0.0271875|0.0265625|0.0296875|0.0278125|0.0284375|0.0271875|0.0284375|0.0309375|0.0309375|  0.02875|0.0340625|0.0296875|  0.03125|0.0359375| 0.671875|0.0640625|             1|[0.0265625,0.0287...|  1.0|[-3.1185966169502...|[0.00195152012128...|       1.0|\n|0.0265625|0.0315625|0.0296875|0.0321875|   0.0275|0.0315625|  0.03625|  0.03375|  0.03375|0.0271875| 0.030625|0.0334375| 0.029375|  0.02625|0.0353125|0.0346875| 0.036875|0.0365625|     0.03|0.0309375|             1|[0.0265625,0.0315...|  1.0|[0.79817172110654...|[0.83150671012612...|       0.0|\n| 0.026875| 0.024375|     0.03|  0.03375|  0.02625|0.0334375|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|             1|[0.026875,0.02437...|  1.0|[0.62128850368963...|[0.77601226114130...|       0.0|\n|0.0271875|   0.0275|  0.02625| 0.034375|0.0321875| 0.031875| 0.031875|0.0346875|0.0340625|0.0303125|  0.03375|  0.03125|0.0290625|   0.0325|0.0328125| 0.029375|0.0253125| 0.039375|0.0528125| 0.056875|             1|[0.0271875,0.0275...|  1.0|[0.40608445771675...|[0.69257149435531...|       0.0|\n|0.0271875| 0.029375|0.0284375| 0.031875|  0.02875| 0.026875|0.0315625|0.0321875|0.0353125|0.0303125|0.0328125|     0.03|0.0278125|     0.03|0.0284375| 0.029375|  0.02875|  0.03125|   0.0325|0.0290625|             0|[0.0271875,0.0293...|  0.0|[0.81354368936530...|[0.83577024808979...|       0.0|\n|   0.0275|0.0315625|  0.03625|  0.03375|  0.03375|0.0271875| 0.030625|0.0334375| 0.029375|  0.02625|0.0353125|0.0346875| 0.036875|0.0365625|     0.03|0.0309375|0.0340625| 0.040625|0.0334375|0.0753125|             1|[0.0275,0.0315625...|  1.0|[0.64263580472637...|[0.78334578477999...|       0.0|\n+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+--------------+--------------------+-----+--------------------+--------------------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#use the GBT model to predict unseen dataset \n",
    "GBT_predictions=GBT_model.transform(testDF)\n",
    "GBT_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789307bb-ba9f-48db-97a6-8ebf5ddb3486",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n|label|prediction|\n+-----+----------+\n|  1.0|       1.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  1.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       0.0|\n+-----+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Lets view the performance of our prediction against our label data\n",
    "GBT_predictions.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0398510-153d-45ba-8af4-b3fd77602ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:27:40 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>', MLflow schema is only supported for scalar Spark types.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.961441\n"
     ]
    }
   ],
   "source": [
    "#Let's evalaute the accuracy of the whole predictions\n",
    "#The accuracy is  0.961441, which means that about 96.1% of the predictions made by our model on the test dataset are correct, this is a good result.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol =\"label\", predictionCol = \"prediction\", metricName = \"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(GBT_predictions)\n",
    "\n",
    "print (\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a8eb80-8f38-498c-851b-cbd98f6f3a66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    " \n",
    "#Create grid builder\n",
    "gbparamGrid =ParamGridBuilder().addGrid(rf_classifier.impurity, [\"variance\", \"entropy\"])\\\n",
    "            .addGrid(GBT_classifier.maxDepth, [3, 5,  7])  \\\n",
    "            .addGrid(GBT_classifier.maxBins,[16,32,64]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6701eb-5803-4af3-bec4-1f0013f9019b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "#Define TrainValidationSplit\n",
    "\n",
    "GBT_tvs=TrainValidationSplit()  \\\n",
    "        .setSeed(100).setTrainRatio(0.75)   \\\n",
    "        .setEstimatorParamMaps(gbparamGrid)   \\\n",
    "        .setEstimator(GBT_classifier)   \\\n",
    "        .setEvaluator(evaluator)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327ea5bd-454d-4651-b9c6-c8717f791679",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/22 10:27:42 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '824d12dde04c444a9cd3690f0227fff0', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n2023/08/22 10:34:14 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: Tuning params should not include params not owned by the tuned estimator, but found a param RandomForestClassifier_1575b5826fb6__impurity\n"
     ]
    }
   ],
   "source": [
    "#Train model using the grid search\n",
    "\n",
    "GBT_gridsearchmodel = GBT_tvs.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77aeaf3-c5a4-4489-9342-9f9fa93206eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the best model:\nMaxDepth Parameter: 7\nImpurity Parameter: entropy\nMaxBins Parameter: 64\n"
     ]
    }
   ],
   "source": [
    "#find the best performing model\n",
    "GBT_bestModel = GBT_gridsearchmodel.bestModel\n",
    " \n",
    "  \n",
    "print(\"Parameters for the best model:\")\n",
    "print(\"MaxDepth Parameter: %g\" %bestModel.getMaxDepth())\n",
    "print(\"Impurity Parameter: %s\" %bestModel.getImpurity())\n",
    "print(\"MaxBins Parameter: %g\" %bestModel.getMaxBins())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526bd855-5495-4fda-8030-5b066f46d59b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the GBTClassifier on the test dataset is: 0.9704504504504504\n"
     ]
    }
   ],
   "source": [
    "#make prediction with the test dataset using the bestModel\n",
    "GBTaccuracy = evaluator.evaluate(GBT_bestModel.transform(testDF))\n",
    "\n",
    "print(\"The accuracy of the GBTClassifier on the test dataset is:\", GBTaccuracy )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3496352732594848,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "faultdata_ml",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
